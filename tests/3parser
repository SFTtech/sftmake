#!/usr/bin/env python
import symlink_3parser

#quick functions for printing colored text easily
#call with no argument to get 'reset' colcode
def colcode(col=""):
	return "\x1b[" + col + "m"

def colprint(text, col):
	print(colcode(col) + text + colcode())

def redprint(text):
	colprint(text, "1;31")

def greenprint(text):
	colprint(text, "32")

def cyanprint(text):
	colprint(text, "36")

"""
testlines = [
	"var1[abc]+=,var2[cde],var3-=value0 value1 'value2'test",
	"var[\"test\"]=val",
	"var[\'test\']=val",
	"var[test\']\'test]=val",
	"var[test\'bla]=val",
	"var[~]=",
	"var[===]=",
	"var[==!]=",
	"var[==<]=",
	"var[=<]=",
	"var[== ==]=",
	"var[= !(=]=",
	"cflags[c == gcc & !(mode ~= 'a b' | mode == rls))]=test",
	"gschichten [ c == 'clang++' & !(!(!( gschicht =		'iee' && asdf	!= \"d f\" ))) ] = value",
	"gschicht[c==''&&lflags=test]=value",
	"var[(c==value]=val",
	"var[version<=20 && (mode <> rls ; ! c == \"ms proprietary compiler\")]=test",
	"var[a=b | c=d & e=f]=value?'ab]ba'",
	" \t\\x41\n \n\\u0041\t\n\t",
	"a'b$'c\"a'b\"c"
	]
"""

class TestInvalidException(Exception):
	pass

def decode_tokens(encoded_tokens):
	decoded_tokens = []
	for enc_token in encoded_tokens:
		if type(enc_token) == tuple:
			decoded_tokens.append(enc_token)
		elif type(enc_token) == str:
			for c in enc_token:
				decoded_tokens.append((str(c),str(c)))
		else:
			raise TestInvalidException("Invalid compare token list")

	return decoded_tokens

def test_tokenizer():
	testlines = [
		("var=value",
			[("IDENTIFIER", "var"), "=", ("IDENTIFIER", "value")]),
		("var[cond=val]=val42ue",
			[("IDENTIFIER", "var"), "[", ("IDENTIFIER", "cond"), "=", ("IDENTIFIER", "val"), "]=", ("IDENTIFIER", "val42ue")]),
		("var[x=$(fun arg     \"  \" ' '  \" \" ' \" \"')]+=\\u262d",
			[("IDENTIFIER", "var"), "[", ("IDENTIFIER", "x"), "=$(", ("IDENTIFIER", "fun"), ("WHITESPACE", " "), ("IDENTIFIER", "arg"),
			("WHITESPACE", "     "), '"', ("WHITESPACE", "  "), '"', ("WHITESPACE", " "), "'", ("WHITESPACE", " "), "'", ("WHITESPACE", "  "),
			'"', ("WHITESPACE", " "), '"', ("WHITESPACE", " "), "'", ("WHITESPACE", " "), '"', ("WHITESPACE", " "), '"', "')]+=", ("LITERAL", "\u262d")]),
		("a'b$'c\"a'b\"c'\"'",
			[("IDENTIFIER", "a"), "'", ("IDENTIFIER", "b"), "$'", ("IDENTIFIER", "c"), '"', ("IDENTIFIER", "a"), "'", ("IDENTIFIER", "b"), '"', ("IDENTIFIER", "c"),
			"'", '"', "'"] ),
		("\\xHH",
			(symlink_3parser.TokenizerXEscapeIllegalCharacterException, 1))
		]

	testcount = len(testlines)
	success = 0
	testnum = 0

	for testline, expected in testlines:
		print("Testing: Tokenizer (" + str(testnum) + "/" + str(testcount) + "): " + testline)

		if type(expected) == tuple:
			#we want the parser to throw an exception
			expected, expected_position = expected
			try:
				tokens = symlink_3parser.tokenize_line(testline)
			except Exception as e:
				try:
					positionstring = " at position " + str(e.pos)
				except:
					positionstring = ""

				if type(e) != expected or e.pos != expected_position:
					redprint("Expected tokenizer to raise exception " + expected.__name__ + " at position " + str(expected_position) + ", but got exception " +
							type(e).__name__ + positionstring + ", text: " + str(e))
				else:
					success += 1
			else:
				redprint("Expected tokenizer to raise exception " + expected.__name__ + " at position " + str(expected_position) + ", but got token list:")
				print(tokens)

		elif type(expected) == list:
			#we want the parser to generate a token list
			#for better usability, tokens may be encoded (special tokens such as (";", ";") and (",", ",") may be represented as a string ";,".
			#decode that.
			try:
				expected_tokens = decode_tokens(expected)
				tokens = symlink_3parser.tokenize_line(testline)

				all_valid = True
				invalid_count = 0
				expected_str = returned_str = ""
				for i, (name, (content, pos)) in enumerate(tokens):
					token = (name, content)
					expected = expected_tokens[i]

					if i != 0:
						expected_str += ", "
						returned_str += ", "

					if token != expected:
						all_valid = False
						invalid_count += 1
						expected_str += colcode("31")
						returned_str += colcode("31")

					expected_str += str(expected) + colcode()
					returned_str += str(token) + colcode()

				if len(tokens) < len(expected_tokens):
					redprint("Additional tokens expected: " + str(len(expected_tokens) - len(tokens)))
					all_valid = False
					expected_str += colcode("33")
					for expected in expected_tokens[len(tokens):]:
						expected_str += ", " + str(expected)
					expected_str += colcode()

				if not all_valid:
					if invalid_count > 0:
						redprint("Invalid tokens: " + str(invalid_count))
					cyanprint("Expected tokens:")
					print(expected_str)
					cyanprint("Tokenizer returned:")
					print(returned_str)
				else:
					success += 1

			except TestInvalidException as e:
				redprint("Test line invalid: " + str(e))
			except IndexError:
				redprint("Expected end of token list, but parser returned additional tokens")
				cyanprint("Tokens so far:")
				print(expected_str)
				cyanprint("Additional tokens:")
				print(list(map(lambda t: (t[0], t[1][0]), tokens[i:])))
			except Exception as e:
				redprint("Got exception of type " + type(e).__name__ + ", text: " + str(e))
				cyanprint("Expected token list:")
				print(expected_tokens)
		else:
			redprint("Test line invalid: 'expected' is neither a tuple, nor a list")

		testnum += 1

	message = "Testing: Tokenizer: " + str(success) + "/" + str(testcount) + " successfull" 
	if success == testcount:
		greenprint(message)
	else:
		redprint(message)

test_tokenizer()
